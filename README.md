# ğŸ§  Mem-LLM - Memory-Enabled AI Assistant

**Memory-enabled AI assistant that remembers conversations using local LLMs**

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue.svg)](https://www.python.org/downloads/)
[![PyPI](https://img.shields.io/pypi/v/mem-llm?label=PyPI)](https://pypi.org/project/mem-llm/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

---

## ğŸ¯ What is it?

A lightweight Python library that adds **persistent memory** to local LLM chatbots. Each user gets their own conversation history that the AI remembers across sessions.

**Perfect for:**

- ğŸ’¬ Customer service chatbots
- ğŸ¤– Personal AI assistants
- ğŸ“ Context-aware applications
- ğŸ¢ Business automation

---

## âš¡ Quick Start

### 1. Install

```bash
pip install mem-llm
```

### 2. Setup Ollama (one-time)

```bash
# Install from: https://ollama.ai/download
ollama serve

# Download model (only 2.5GB)
ollama pull granite4:tiny-h
```

### 3. Use in Python

```python
from mem_llm import MemAgent

# Create agent
agent = MemAgent()
agent.set_user("john")

# Chat - it remembers!
agent.chat("My name is John")
agent.chat("What's my name?")  # â†’ "Your name is John"
```

### 4. Or Use CLI

```bash
# Interactive chat
mem-llm chat --user john

# Check system status
mem-llm check

# View statistics
mem-llm stats
```

---

## âœ¨ Key Features

- ğŸ§  **Persistent Memory**: Remembers conversations across sessions
- ğŸ’¬ **Context Awareness**: Uses conversation history for relevant responses
- ğŸ  **Local & Private**: Runs entirely on your machine
- ğŸš€ **Lightweight**: Works with small local models (~2.5GB)
- ğŸ¯ **Multi-Backend**: JSON and SQL memory storage options
- ğŸ“š **Knowledge Base**: Config-free document integration
- ğŸŒ **Multi-language**: Works with any language
- ğŸ–¥ï¸ **CLI Tool**: Built-in command-line interface

---

## ğŸ”„ Memory Backend Comparison

| Feature | JSON Mode | SQL Mode |
|---------|-----------|----------|
| **Setup** | âœ… Zero config | âš™ï¸ Minimal config |
| **Conversation Memory** | âœ… Yes | âœ… Yes |
| **User Profiles** | âœ… Yes | âœ… Yes |
| **Knowledge Base** | âŒ No | âœ… Yes |
| **Advanced Search** | âŒ No | âœ… Yes |
| **Performance** | â­â­ Good | â­â­â­ Excellent |
| **Best For** | ğŸ  Personal use | ğŸ¢ Business use |

---

## ğŸ“š Documentation

- [ğŸ“– Full Documentation](Memory%20LLM/README.md)
- [ğŸš€ Quick Start Guide](Memory%20LLM/QUICKSTART.md)
- [ğŸ”— Integration Guide](Memory%20LLM/INTEGRATION_GUIDE.md)
- [ğŸ“ Changelog](Memory%20LLM/CHANGELOG.md)

---

## ğŸ› ï¸ Development

### Setup Development Environment

```bash
cd "Memory LLM"
pip install -r requirements-dev.txt
```

### Run Tests

```bash
pytest tests/
```

---

## ğŸ“„ License

MIT License - feel free to use in personal and commercial projects!

---

## ğŸ”— Links

- **PyPI**: <https://pypi.org/project/mem-llm/>
- **GitHub**: <https://github.com/emredeveloper/Mem-LLM>
- **Ollama**: <https://ollama.ai/>

---

## ğŸŒŸ Star us on GitHub

If you find this useful, give us a â­ on [GitHub](https://github.com/emredeveloper/Mem-LLM)

---

Made with â¤ï¸ by [C. Emre KarataÅŸ](https://github.com/emredeveloper)
