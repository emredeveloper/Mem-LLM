# ğŸ§  mem-llm# ğŸ§  mem-llm



**Memory-enabled AI assistant that remembers conversations using local LLMs****Memory-enabled AI assistant that remembers conversations using local LLMs**



[![Python](https://img.shields.io/badge/Python-3.8%2B-blue.svg)](https://www.python.org/downloads/)[![Python](https://img.shields.io/badge/Python-3.8%2B-blue.svg)](https://www.python.org/downloads/)

[![PyPI](https://img.shields.io/pypi/v/mem-llm?label=PyPI)](https://pypi.org/project/mem-llm/)

[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)[![PyPI](https://img.shields.io/pypi/v/mem-llm?label=PyPI)](https://pypi.org/project/mem-llm/)



---[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)



## ğŸ¯ What is mem-llm?



`mem-llm` is a lightweight Python library that adds **persistent memory** to your local LLM chatbots. Each user gets their own conversation history that persists across sessions, enabling truly personalized AI interactions.---[![Python](https://img.shields.io/badge/Python-3.8%2B-blue.svg)](https://www.python.org/downloads/)**Memory-enabled AI assistant that remembers conversations using local LLMs****Memory-enabled AI assistant that remembers conversations using local LLMs**



**Key Use Cases:**

- ğŸ’¬ Customer service bots with conversation history

- ğŸ¤– Personal assistants that remember your preferences## ğŸ¯ What is mem-llm?[![PyPI](https://img.shields.io/pypi/v/mem-llm?label=PyPI)](https://pypi.org/project/mem-llm/)

- ğŸ“ Context-aware applications

- ğŸ¢ Business automation solutions



---`mem-llm` is a lightweight Python library that adds **persistent memory** to your local LLM chatbots. Each user gets their own conversation history that persists across sessions.[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)



## âš¡ Quick Start



### 1. Install the package**Use Cases:**



```bash- ğŸ’¬ Customer service bots

pip install mem-llm

```- ğŸ¤– Personal assistants---[![Python](https://img.shields.io/badge/Python-3.8%2B-blue.svg)](https://www.python.org/downloads/)[![Python](https://img.shields.io/badge/Python-3.8%2B-blue.svg)](https://www.python.org/downloads/)



### 2. Start Ollama and download a model (one-time setup)- ğŸ“ Context-aware applications



```bash- ğŸ¢ Business automation solutions

# Start Ollama service

ollama serve



# Download a lightweight model (~2.5GB)---## ğŸ¯ What is mem-llm?[![PyPI](https://img.shields.io/pypi/v/mem-llm?label=PyPI)](https://pypi.org/project/mem-llm/)[![PyPI](https://img.shields.io/pypi/v/mem-llm?label=PyPI)](https://pypi.org/project/mem-llm/)

ollama pull granite4:tiny-h

```



> ğŸ’¡ Keep `ollama serve` running in one terminal, run your Python code in another.## âš¡ Quick Start



### 3. Create your first memory-enabled agent



```python### 1. Install the package`mem-llm` is a lightweight Python library that adds **persistent memory** to your local LLM chatbots. Each user gets their own conversation history that persists across sessions.[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

from mem_llm import MemAgent



# Create agent

agent = MemAgent()```bash



# Set user (each user gets separate memory)pip install mem-llm

agent.set_user("john")

```**Use Cases:**

# Chat with memory

response = agent.chat("My name is John")

print(response)

### 2. Start Ollama and download a model (one-time setup)- ğŸ’¬ Customer service bots

# Later conversation - memory is retained

response = agent.chat("What's my name?")

print(response)  # Output: "Your name is John"

``````bash- ğŸ¤– Personal assistants------



### 4. Verify your setup (optional)# Start Ollama service



```bashollama serve- ğŸ“ Context-aware applications

# Using CLI

mem-llm check



# Or in Python# Download lightweight model (~2.5GB)- ğŸ¢ Business automation solutions

agent.check_setup()

```ollama pull granite4:tiny-h



---```



## ğŸ’¡ Features



| Feature | Description |> ğŸ’¡ Keep `ollama serve` running in one terminal, run your Python code in another.---## ğŸ¯ What is mem-llm?## ğŸ“š Ä°Ã§indekiler

|---------|-------------|

| ğŸ§  **Persistent Memory** | Remembers each user's conversation history |

| ğŸ‘¥ **Multi-User Support** | Separate memory for each user |

| ğŸ”’ **100% Private** | Completely local, no cloud/API needed |### 3. Create your first agent

| âš¡ **Fast & Lightweight** | SQLite or JSON storage options |

| ğŸ¯ **Simple API** | Get started with just 3 lines of code |

| ğŸ“š **Knowledge Base** | Optional document integration |

| ğŸŒ **Multi-Language** | Works with any language |```python## âš¡ Quick Start

| ğŸ› ï¸ **CLI Tool** | Built-in command-line interface |

from mem_llm import MemAgent

---



## ğŸ”„ Memory Backend Comparison

# Create agent in one line

Choose the right backend for your needs:

agent = MemAgent()### 1. Install the package`mem-llm` is a lightweight Python library that adds **persistent memory** to your local LLM chatbots. Each user gets their own conversation history that persists across sessions.- [ğŸ¯ mem-llm nedir?](#-mem-llm-nedir)

| Feature | JSON Mode | SQL Mode |

|---------|-----------|----------|

| **Setup** | âœ… Zero config | âš™ï¸ Minimal config |

| **Conversation Memory** | âœ… Yes | âœ… Yes |# Set user (each user gets separate memory)

| **User Profiles** | âœ… Yes | âœ… Yes |

| **Knowledge Base** | âŒ No | âœ… Yes |agent.set_user("john")

| **Advanced Search** | âŒ No | âœ… Yes |

| **Multi-User Performance** | â­â­ Good | â­â­â­ Excellent |```bash- [âš¡ HÄ±zlÄ± baÅŸlangÄ±Ã§](#-hÄ±zlÄ±-baÅŸlangÄ±Ã§)

| **Data Queries** | âŒ Limited | âœ… Full SQL support |

| **Best For** | ğŸ  Personal use | ğŸ¢ Business/Production |# Chat with memory!



**Recommendation:**response = agent.chat("My name is John")pip install mem-llm

- **JSON Mode**: Perfect for personal assistants and quick prototypes

- **SQL Mode**: Ideal for customer service, multi-user apps, and production environmentsprint(response)



---```**Use Cases:**- [ğŸ§‘â€ğŸ« Tutorial](#-tutorial)



## ğŸ“– Usage Examplesresponse = agent.chat("What's my name?")



### Example 1: Basic Conversation with Memoryprint(response)  # Output: "Your name is John"



```python```

from mem_llm import MemAgent

### 2. Start Ollama and download a model (one-time setup)- ğŸ’¬ Customer service bots- [ğŸ’¡ Ã–zellikler](#-Ã¶zellikler)

# Create agent

agent = MemAgent()### 4. Verify your setup (optional)

agent.set_user("alice")



# First conversation

response = agent.chat("I love pizza")```bash

print(response)

# Using CLI```bash- ğŸ¤– Personal assistants- [ğŸ“– KullanÄ±m Ã¶rnekleri](#-kullanÄ±m-Ã¶rnekleri)

# Memory test - bot remembers

response = agent.chat("What's my favorite food?")mem-llm check

print(response)  # Output: "Your favorite food is pizza!"

```# Start Ollama service



### Example 2: Multi-User Support# Or in Python



```pythonagent.check_setup()ollama serve- ğŸ“ Context-aware applications- [ğŸ”§ YapÄ±landÄ±rma seÃ§enekleri](#-yapÄ±landÄ±rma-seÃ§enekleri)

from mem_llm import MemAgent

```

agent = MemAgent()



# Customer 1

agent.set_user("customer_john")---

agent.chat("My order #12345 is delayed")

# Download lightweight model (~2.5GB)- ğŸ¢ Business automation solutions- [ğŸ—‚ Bilgi tabanÄ± ve dokÃ¼manlardan yapÄ±landÄ±rma](#-bilgi-tabanÄ±-ve-dokÃ¼manlardan-yapÄ±landÄ±rma)

# Customer 2 - SEPARATE MEMORY

agent.set_user("customer_sarah")## ğŸ’¡ Features

agent.chat("I want to return item #67890")

ollama pull granite4:tiny-h

# Back to Customer 1 - remembers previous conversation

agent.set_user("customer_john")| Feature | Description |

response = agent.chat("What was my order number?")

print(response)  # Output: "Your order number is #12345"|---------|-------------|```- [ğŸ”¥ Desteklenen modeller](#-desteklenen-modeller)

```

| ğŸ§  **Memory** | Remembers each user's conversation history |

### Example 3: Multi-Language Support

| ğŸ‘¥ **Multi-user** | Separate memory for each user |

```python

from mem_llm import MemAgent| ğŸ”’ **Privacy** | 100% local, no cloud/API needed |



agent = MemAgent()| âš¡ **Fast** | Lightweight SQLite/JSON storage |> ğŸ’¡ Keep `ollama serve` running in one terminal, run your Python code in another.---- [ğŸ“¦ Gereksinimler](#-gereksinimler)

agent.set_user("ahmet")

| ğŸ¯ **Simple** | 3 lines of code to get started |

# Turkish conversation

agent.chat("Benim adÄ±m Ahmet ve Ä°stanbul'da yaÅŸÄ±yorum")| ğŸ“š **Knowledge Base** | Config-free document integration |

response = agent.chat("Nerede yaÅŸÄ±yorum?")

print(response)  # Output: "Ä°stanbul'da yaÅŸÄ±yorsunuz"| ğŸŒ **Multi-language** | Works with any language |



response = agent.chat("AdÄ±mÄ± hatÄ±rlÄ±yor musun?")| ğŸ› ï¸ **CLI Tool** | Built-in command-line interface |### 3. Create your first agent- [ğŸ› SÄ±k karÅŸÄ±laÅŸÄ±lan problemler](#-sÄ±k-karÅŸÄ±laÅŸÄ±lan-problemler)

print(response)  # Output: "Evet, adÄ±nÄ±z Ahmet!"

```



### Example 4: User Profile Extraction---



```python

from mem_llm import MemAgent

## ğŸ”„ Memory Backend Comparison```python## âš¡ Quick Start

agent = MemAgent()

agent.set_user("alice")



# Have natural conversationsChoose the right backend for your needs:from mem_llm import MemAgent

agent.chat("My name is Alice and I'm 28 years old")

agent.chat("I live in New York City")

agent.chat("I work as a software engineer")

agent.chat("My favorite food is pizza")| Feature | JSON Mode | SQL Mode |---



# Extract profile automatically|---------|-----------|----------|

profile = agent.get_user_profile()

print(profile)| **Setup** | âœ… Zero config | âš™ï¸ Minimal config |# Create agent in one line

# Output: {'name': 'Alice', 'age': 28, 'location': 'New York City', ...}

```| **Conversation Memory** | âœ… Yes | âœ… Yes |



---| **User Profiles** | âœ… Yes | âœ… Yes |agent = MemAgent()### 1. Install the package



## ğŸ”§ Configuration Options| **Knowledge Base** | âŒ No | âœ… Yes |



### JSON Memory (Simple, Default)| **Advanced Search** | âŒ No | âœ… Yes |



```python| **Multi-user Performance** | â­â­ Good | â­â­â­ Excellent |

agent = MemAgent(

    model="granite4:tiny-h",| **Data Queries** | âŒ Limited | âœ… Full SQL |# Set user (each user gets separate memory)## ğŸ¯ mem-llm nedir?

    use_sql=False,  # JSON file-based memory

    memory_dir="memories"| **Best For** | ğŸ  Personal use | ğŸ¢ Business use |

)

```agent.set_user("john")



### SQL Memory (Advanced, Recommended for Production)**Recommendation:**



```python- **JSON Mode**: Perfect for personal assistants and quick prototypes```bash

agent = MemAgent(

    model="granite4:tiny-h",- **SQL Mode**: Ideal for customer service, multi-user apps, and production

    use_sql=True,  # SQLite-based memory

    memory_dir="memories.db"# Chat with memory!

)

```---



### Custom Configurationresponse = agent.chat("My name is John")pip install mem-llm`mem-llm`, yerel bir LLM ile Ã§alÄ±ÅŸan sohbet botlarÄ±nÄ±za **kalÄ±cÄ± hafÄ±za** kazandÄ±ran hafif bir Python kÃ¼tÃ¼phanesidir. Her kullanÄ±cÄ± iÃ§in ayrÄ± bir konuÅŸma geÃ§miÅŸi tutulur ve yapay zeka bu geÃ§miÅŸi bir sonraki oturumda otomatik olarak kullanÄ±r.



```python## ğŸ“– Usage Examples

agent = MemAgent(

    model="llama2",  # Any Ollama modelprint(response)

    ollama_url="http://localhost:11434",

    check_connection=True  # Verify setup on startup### Example 1: Basic Conversation with Memory

)

``````



---```python



## ğŸ› ï¸ Command Line Interfacefrom mem_llm import MemAgentresponse = agent.chat("What's my name?")



```bash

# Start interactive chat

mem-llm chat --user john# Create agentprint(response)  # Output: "Your name is John"**Nerelerde kullanÄ±labilir?**



# Check system statusagent = MemAgent()

mem-llm check

agent.set_user("alice")```

# View statistics

mem-llm stats



# Export user data# First conversation### 2. Start Ollama and download a model (one-time setup)- ğŸ’¬ MÃ¼ÅŸteri hizmetleri botlarÄ±

mem-llm export john --format json

response1 = agent.chat("I love pizza")

# Clear user data

mem-llm clear johnprint(response1)### 4. Verify your setup (optional)



# Get help

mem-llm --help

```# Memory test - bot remembers!- ğŸ¤– KiÅŸisel asistanlar



**Available CLI Commands:**response2 = agent.chat("What's my favorite food?")



| Command | Description | Example |print(response2)  # Output: "Your favorite food is pizza!"```bash

|---------|-------------|---------|

| `chat` | Interactive chat session | `mem-llm chat --user alice` |```

| `check` | Verify system setup | `mem-llm check` |

| `stats` | Show statistics | `mem-llm stats --user john` |# Using CLI```bash- ğŸ“ BaÄŸlama duyarlÄ± uygulamalar

| `export` | Export user data | `mem-llm export john` |

| `clear` | Delete user data | `mem-llm clear john` |### Example 2: Multi-User Support



---mem-llm check



## ğŸ“š API Reference```python



### MemAgent Classfrom mem_llm import MemAgent# Start Ollama service- ğŸ¢ Ä°ÅŸ sÃ¼reÃ§lerini otomatikleÅŸtiren Ã§Ã¶zÃ¼mler



```python

# Initialize

agent = MemAgent(agent = MemAgent()# Or in Python

    model="granite4:tiny-h",

    use_sql=True,

    memory_dir=None,

    ollama_url="http://localhost:11434",# Customer 1agent.check_setup()ollama serve

    check_connection=False

)agent.set_user("customer_john")



# Set active useragent.chat("My order #12345 is delayed")```

agent.set_user(user_id: str, name: Optional[str] = None)



# Chat (returns response string)

response = agent.chat(message: str, metadata: Optional[Dict] = None) -> str# Customer 2 - SEPARATE MEMORY!---



# Get user profile (auto-extracted from conversations)agent.set_user("customer_sarah")

profile = agent.get_user_profile(user_id: Optional[str] = None) -> Dict

agent.chat("I want to return item #67890")---

# System check

status = agent.check_setup() -> Dict

```

# Back to Customer 1 - remembers previous conversation!# Download lightweight model (~2.5GB)

---

agent.set_user("customer_john")

## ğŸ”¥ Supported Models

response = agent.chat("What was my order number?")## ğŸ’¡ Features

Works with any [Ollama](https://ollama.ai/) model. Recommended models:

print(response)  # Output: "Your order number is #12345"

| Model | Size | Speed | Quality | Best For |

|-------|------|-------|---------|----------|```ollama pull granite4:tiny-h## âš¡ HÄ±zlÄ± baÅŸlangÄ±Ã§

| `granite4:tiny-h` | 2.5GB | âš¡âš¡âš¡ | â­â­ | Quick testing |

| `llama2` | 4GB | âš¡âš¡ | â­â­â­ | General use |

| `mistral` | 4GB | âš¡âš¡ | â­â­â­â­ | Balanced performance |

| `llama3` | 5GB | âš¡ | â­â­â­â­â­ | Best quality |### Example 3: Multi-language Support| Feature | Description |



```bash

# Download a model

ollama pull <model-name>```python|---------|-------------|```



# List installed modelsfrom mem_llm import MemAgent

ollama list

```| ğŸ§  **Memory** | Remembers each user's conversation history |



---agent = MemAgent()



## ğŸ“¦ Requirementsagent.set_user("ahmet")| ğŸ‘¥ **Multi-user** | Separate memory for each user |### 0. Gereksinimleri kontrol edin



- Python 3.8+

- [Ollama](https://ollama.ai/) (for local LLM)

- Minimum 4GB RAM# Turkish conversation| ğŸ”’ **Privacy** | 100% local, no cloud/API needed |

- 5GB disk space

agent.chat("Benim adÄ±m Ahmet ve Ä°stanbul'da yaÅŸÄ±yorum")

**Python Dependencies (auto-installed):**

- `requests >= 2.31.0`agent.chat("Nerede yaÅŸÄ±yorum?")  # â†’ "Ä°stanbul'da yaÅŸÄ±yorsunuz"| âš¡ **Fast** | Lightweight SQLite/JSON storage |> ğŸ’¡ Keep `ollama serve` running in one terminal, run your Python code in another.

- `pyyaml >= 6.0.1`

- `click >= 8.1.0`agent.chat("AdÄ±mÄ± hatÄ±rlÄ±yor musun?")  # â†’ "Evet, adÄ±nÄ±z Ahmet!"



---```| ğŸ¯ **Simple** | 3 lines of code to get started |



## ğŸ› Troubleshooting



### Ollama not running?### Example 4: User Profile Extraction| ğŸ“š **Knowledge Base** | Config-free document integration |- Python 3.8 veya Ã¼zeri



```bash

ollama serve

``````python| ğŸŒ **Multi-language** | Works with any language |



### Model not found error?from mem_llm import MemAgent



```bash| ğŸ› ï¸ **CLI Tool** | Built-in command-line interface |### 3. Create your first agent- [Ollama](https://ollama.ai/) kurulu ve Ã§alÄ±ÅŸÄ±r durumda

# Download the model

ollama pull granite4:tiny-hagent = MemAgent()



# Check installed modelsagent.set_user("alice")

ollama list

```



### Connection error?# Have natural conversations---- En az 4GB RAM ve 5GB disk alanÄ±



```bashagent.chat("My name is Alice and I'm 28 years old")

# Check if Ollama is running

curl http://localhost:11434agent.chat("I live in New York City")



# Restart Ollamaagent.chat("I work as a software engineer")

ollama serve

```agent.chat("My favorite food is pizza")## ğŸ”„ Memory Backend Comparison```python



### Import error?



```bash# Extract profile automatically

# Upgrade to latest version

pip install --upgrade mem-llmprofile = agent.get_user_profile()

```

print(profile)Choose the right backend for your needs:from mem_llm import MemAgent### 1. Paketi yÃ¼kleyin

> ğŸ’¡ If issues persist, run `mem-llm check` or `agent.check_setup()` and share the output when opening an issue.

# Output: {'name': 'Alice', 'age': 28, 'location': 'NYC', ...}

---

```

## ğŸ“„ License



MIT License - Free to use in personal and commercial projects.

---| Feature | JSON Mode | SQL Mode |

---



## ğŸ”— Links

## ğŸ”§ Configuration Options|---------|-----------|----------|

- **PyPI:** https://pypi.org/project/mem-llm/

- **GitHub:** https://github.com/emredeveloper/Mem-LLM

- **Ollama:** https://ollama.ai/

- **Documentation:** [GitHub Wiki](https://github.com/emredeveloper/Mem-LLM/wiki)### JSON Memory (Simple, Default)| **Setup** | âœ… Zero config | âš™ï¸ Minimal config |# Create agent in one line```bash



---



## ğŸŒŸ Support Us```python| **Conversation Memory** | âœ… Yes | âœ… Yes |



If you find this project useful, please â­ [star it on GitHub](https://github.com/emredeveloper/Mem-LLM)!agent = MemAgent(



---    model="granite4:tiny-h",| **User Profiles** | âœ… Yes | âœ… Yes |agent = MemAgent()pip install mem-llm==1.0.7



## ğŸ¤ Contributing    use_sql=False,  # JSON file-based memory



Contributions are welcome! Please feel free to submit a Pull Request.    memory_dir="memories"| **Knowledge Base** | âŒ No | âœ… Yes |



1. Fork the repository)

2. Create your feature branch (`git checkout -b feature/AmazingFeature`)

3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)```| **Advanced Search** | âŒ No | âœ… Yes |```

4. Push to the branch (`git push origin feature/AmazingFeature`)

5. Open a Pull Request



---### SQL Memory (Advanced, Recommended for Production)| **Multi-user Performance** | â­â­ Good | â­â­â­ Excellent |



<div align="center">

Made with â¤ï¸ by <a href="https://github.com/emredeveloper">C. Emre KarataÅŸ</a>

</div>```python| **Data Queries** | âŒ Limited | âœ… Full SQL |# Set user (each user gets separate memory)


agent = MemAgent(

    model="granite4:tiny-h",| **Best For** | ğŸ  Personal use | ğŸ¢ Business use |

    use_sql=True,  # SQLite-based memory

    memory_dir="memories.db"agent.set_user("john")### 2. Ollama'yÄ± baÅŸlatÄ±n ve modeli indirin (tek seferlik)

)

```**Recommendation:**



### Custom Configuration- **JSON Mode**: Perfect for personal assistants and quick prototypes



```python- **SQL Mode**: Ideal for customer service, multi-user apps, and production

agent = MemAgent(

    model="llama2",  # Any Ollama model# Chat with memory!```bash

    ollama_url="http://localhost:11434",

    check_connection=True  # Verify setup on startup---

)

```response = agent.chat("My name is John")# Ollama servisini baÅŸlatÄ±n



---## ğŸ“– Usage Examples



## ğŸ› ï¸ Command Line Interfaceprint(response)ollama serve



```bash### Example 1: Basic Conversation with Memory

# Start interactive chat

mem-llm chat --user john



# Check system status```python

mem-llm check

from mem_llm import MemAgentresponse = agent.chat("What's my name?")# YaklaÅŸÄ±k 2.5GB'lÄ±k hafif modeli indirin

# View statistics

mem-llm stats



# Export user data# Create agentprint(response)  # Output: "Your name is John"ollama pull granite4:tiny-h

mem-llm export john --format json

agent = MemAgent()

# Clear user data

mem-llm clear johnagent.set_user("alice")``````



# Get help

mem-llm --help

```# First conversation



**Available CLI Commands:**response1 = agent.chat("I love pizza")



| Command | Description | Example |print(response1)### 4. Verify your setup (optional)> ğŸ’¡ Ollama `serve` komutu terminalde aÃ§Ä±k kalmalÄ±dÄ±r. Yeni bir terminal sekmesinde Python kodunu Ã§alÄ±ÅŸtÄ±rabilirsiniz.

|---------|-------------|---------|

| `chat` | Interactive chat session | `mem-llm chat --user alice` |

| `check` | Verify system setup | `mem-llm check` |

| `stats` | Show statistics | `mem-llm stats --user john` |# Memory test - bot remembers!

| `export` | Export user data | `mem-llm export john` |

| `clear` | Delete user data | `mem-llm clear john` |response2 = agent.chat("What's my favorite food?")



---print(response2)  # Output: "Your favorite food is pizza!"```bash### 3. Ä°lk ajanÄ±nÄ±zÄ± Ã§alÄ±ÅŸtÄ±rÄ±n



## ğŸ“š API Reference```



### MemAgent Class# Using CLI



```python### Example 2: Multi-User Support

# Initialize

agent = MemAgent(mem-llm check```python

    model="granite4:tiny-h",

    use_sql=True,```python

    memory_dir=None,

    ollama_url="http://localhost:11434",from mem_llm import MemAgentfrom mem_llm import MemAgent

    check_connection=False

)



# Set active useragent = MemAgent()# Or in Python

agent.set_user(user_id: str, name: Optional[str] = None)



# Chat (returns response string)

response = agent.chat(message: str, metadata: Optional[Dict] = None) -> str# Customer 1agent.check_setup()# Tek satÄ±rda ajan oluÅŸturun



# Get user profile (auto-extracted from conversations)agent.set_user("customer_john")

profile = agent.get_user_profile(user_id: Optional[str] = None) -> Dict

agent.chat("My order #12345 is delayed")```agent = MemAgent()

# System check

status = agent.check_setup() -> Dict

```

# Customer 2 - SEPARATE MEMORY!

---

agent.set_user("customer_sarah")

## ğŸ”¥ Supported Models

agent.chat("I want to return item #67890")---# KullanÄ±cÄ±yÄ± belirleyin (her kullanÄ±cÄ± iÃ§in ayrÄ± hafÄ±za tutulur)

Works with any [Ollama](https://ollama.ai/) model. Recommended models:



| Model | Size | Speed | Quality | Best For |

|-------|------|-------|---------|----------|# Back to Customer 1 - remembers previous conversation!agent.set_user("john")

| `granite4:tiny-h` | 2.5GB | âš¡âš¡âš¡ | â­â­ | Quick testing |

| `llama2` | 4GB | âš¡âš¡ | â­â­â­ | General use |agent.set_user("customer_john")

| `mistral` | 4GB | âš¡âš¡ | â­â­â­â­ | Balanced |

| `llama3` | 5GB | âš¡ | â­â­â­â­â­ | Best quality |response = agent.chat("What was my order number?")## ğŸ’¡ Features



```bashprint(response)  # Output: "Your order number is #12345"

# Download a model

ollama pull <model-name>```# Sohbet edin - hafÄ±za devrede!



# List installed models

ollama list

```### Example 3: Multi-language Support| Feature | Description |agent.chat("My name is John")



---



## ğŸ“¦ Requirements```python|---------|-------------|agent.chat("What's my name?")  # â†’ "Your name is John"



- Python 3.8+from mem_llm import MemAgent

- [Ollama](https://ollama.ai/) (for LLM)

- Minimum 4GB RAM| ğŸ§  **Memory** | Remembers each user's conversation history |```

- 5GB disk space

agent = MemAgent()

**Python Dependencies (auto-installed):**

- `requests >= 2.31.0`agent.set_user("ahmet")| ğŸ‘¥ **Multi-user** | Separate memory for each user |

- `pyyaml >= 6.0.1`

- `click >= 8.1.0`



---# Turkish conversation| ğŸ”’ **Privacy** | 100% local, no cloud/API needed |### 4. Kurulumunuzu doÄŸrulayÄ±n (isteÄŸe baÄŸlÄ±)



## ğŸ› Troubleshootingagent.chat("Benim adÄ±m Ahmet ve Ä°stanbul'da yaÅŸÄ±yorum")



### Ollama not running?agent.chat("Nerede yaÅŸÄ±yorum?")  # â†’ "Ä°stanbul'da yaÅŸÄ±yorsunuz"| âš¡ **Fast** | Lightweight SQLite/JSON storage |



```bashagent.chat("AdÄ±mÄ± hatÄ±rlÄ±yor musun?")  # â†’ "Evet, adÄ±nÄ±z Ahmet!"

ollama serve

``````| ğŸ¯ **Simple** | 3 lines of code to get started |```python



### Model not found error?



```bash### Example 4: User Profile Extraction| ğŸ“š **Knowledge Base** | Load information from documents |agent.check_setup()

# Download the model

ollama pull granite4:tiny-h



# Check installed models```python| ğŸŒ **Multi-language** | Works with any language (Turkish, English, etc.) |# {'ollama': 'running', 'model': 'granite4:tiny-h', 'memory_backend': 'sql', ...}

ollama list

```from mem_llm import MemAgent



### Connection error?| ğŸ› ï¸ **CLI Tool** | Built-in command-line interface |```



```bashagent = MemAgent()

# Check if Ollama is running

curl http://localhost:11434agent.set_user("alice")



# Restart Ollama

ollama serve

```# Have natural conversations---<<<<<<< HEAD



### Import error?agent.chat("My name is Alice and I'm 28 years old")



```bashagent.chat("I live in New York City")| Feature | Description |

# Upgrade to latest version

pip install --upgrade mem-llmagent.chat("I work as a software engineer")

```

agent.chat("My favorite food is pizza")## ğŸ“– Usage Examples|---------|-------------|

> If issues persist, run `mem-llm check` or `agent.check_setup()` and share the output when opening an issue.



---

# Extract profile automatically| ğŸ§  **Memory** | Remembers each user's conversation history |

## ğŸ“„ License

profile = agent.get_user_profile()

MIT License - Free to use in personal and commercial projects.

print(profile)### Example 1: Basic Conversation with Memory| ğŸ‘¥ **Multi-user** | Separate memory for each user |

---

# Output: {'name': 'Alice', 'age': 28, 'location': 'NYC', ...}

## ğŸ”— Links

```| ğŸ”’ **Privacy** | 100% local, no cloud/API needed |

- **PyPI:** https://pypi.org/project/mem-llm/

- **GitHub:** https://github.com/emredeveloper/Mem-LLM

- **Ollama:** https://ollama.ai/

- **Documentation:** [GitHub Wiki](https://github.com/emredeveloper/Mem-LLM/wiki)---```python| âš¡ **Fast** | Lightweight SQLite/JSON storage |



---



## ğŸŒŸ Support Us## ğŸ”§ Configuration Optionsfrom mem_llm import MemAgent| ğŸ¯ **Simple** | 3 lines of code to get started |



If you find this project useful, please â­ [star it on GitHub](https://github.com/emredeveloper/Mem-LLM)!



---### JSON Memory (Simple, Default)| ğŸ“š **Knowledge Base** | Config-free document integration |



## ğŸ¤ Contributing



Contributions are welcome! Please feel free to submit a Pull Request.```python# Create agent| ğŸŒ **Multi-language** | Works with any language |



1. Fork the repositoryagent = MemAgent(

2. Create your feature branch (`git checkout -b feature/AmazingFeature`)

3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)    model="granite4:tiny-h",print("ğŸ¤– Creating AI agent...")| ğŸ› ï¸ **CLI Tool** | Built-in command-line interface |

4. Push to the branch (`git push origin feature/AmazingFeature`)

5. Open a Pull Request    use_sql=False,  # JSON file-based memory



---    memory_dir="memories"agent = MemAgent()



<div align="center">)

Made with â¤ï¸ by <a href="https://github.com/emredeveloper">C. Emre KarataÅŸ</a>

</div>```---




### SQL Memory (Advanced, Recommended for Production)# Set user



```pythonprint("ğŸ‘¤ Setting user: alice\n")## ğŸ”„ Memory Backend Comparison

agent = MemAgent(

    model="granite4:tiny-h",agent.set_user("alice")

    use_sql=True,  # SQLite-based memory

    memory_dir="memories.db"Choose the right backend for your needs:

)

```# First conversation



### Custom Configurationprint("ğŸ’¬ User: I love pizza")| Feature | JSON Mode | SQL Mode |



```pythonresponse1 = agent.chat("I love pizza")|---------|-----------|----------|

agent = MemAgent(

    model="llama2",  # Any Ollama modelprint(f"ğŸ¤– Bot: {response1}\n")| **Setup** | âœ… Zero config | âš™ï¸ Minimal config |

    ollama_url="http://localhost:11434",

    check_connection=True  # Verify setup on startup| **Conversation Memory** | âœ… Yes | âœ… Yes |

)

```# Memory test - bot remembers!| **User Profiles** | âœ… Yes | âœ… Yes |



---print("ğŸ’¬ User: What's my favorite food?")| **Knowledge Base** | âŒ No | âœ… Yes |



## ğŸ› ï¸ Command Line Interfaceresponse2 = agent.chat("What's my favorite food?")| **Advanced Search** | âŒ No | âœ… Yes |



```bashprint(f"ğŸ¤– Bot: {response2}")| **Multi-user Performance** | â­â­ Good | â­â­â­ Excellent |

# Start interactive chat

mem-llm chat --user john```| **Data Queries** | âŒ Limited | âœ… Full SQL |



# Check system status| **Best For** | ğŸ  Personal use | ğŸ¢ Business use |

mem-llm check

**Output:**

# View statistics

mem-llm stats```**Recommendation:**



# Export user datağŸ¤– Creating AI agent...- **JSON Mode**: Perfect for personal assistants and quick prototypes

mem-llm export john --format json

ğŸ‘¤ Setting user: alice- **SQL Mode**: Ideal for customer service, multi-user apps, and production

# Clear user data

mem-llm clear john=======



# Get helpğŸ’¬ User: I love pizzaKurulum sÄ±rasÄ±nda sorun yaÅŸarsanÄ±z [ğŸ› SÄ±k karÅŸÄ±laÅŸÄ±lan problemler](#-sÄ±k-karÅŸÄ±laÅŸÄ±lan-problemler) bÃ¶lÃ¼mÃ¼ne gÃ¶z atÄ±n.

mem-llm --help

```ğŸ¤– Bot: That's great! Pizza is a popular choice...>>>>>>> f002396c8c531e4cde33d19ac6a755494b1b30cd



**Available CLI Commands:**



| Command | Description | Example |ğŸ’¬ User: What's my favorite food?---

|---------|-------------|---------|

| `chat` | Interactive chat session | `mem-llm chat --user alice` |ğŸ¤– Bot: Based on our conversation, your favorite food is pizza!

| `check` | Verify system setup | `mem-llm check` |

| `stats` | Show statistics | `mem-llm stats --user john` |```## ğŸ’¡ Ã–zellikler

| `export` | Export user data | `mem-llm export john` |

| `clear` | Delete user data | `mem-llm clear john` |



------<<<<<<< HEAD



## ğŸ“š API Reference### Command Line Interface (CLI)



### MemAgent Class### Example 2: Multi-User Support



```pythonThe easiest way to get started:

# Initialize

agent = MemAgent(```python

    model="granite4:tiny-h",

    use_sql=True,from mem_llm import MemAgent```bash

    memory_dir=None,

    ollama_url="http://localhost:11434",# Install with CLI support

    check_connection=False

)agent = MemAgent()pip install mem-llm



# Set active user

agent.set_user(user_id: str, name: Optional[str] = None)

# Customer 1# Start interactive chat

# Chat (returns response string)

response = agent.chat(message: str, metadata: Optional[Dict] = None) -> strprint("=" * 60)mem-llm chat --user john



# Get user profile (auto-extracted from conversations)print("ğŸ‘¤ Customer 1: John")

profile = agent.get_user_profile(user_id: Optional[str] = None) -> Dict

print("=" * 60)# Check system status

# System check

status = agent.check_setup() -> Dictagent.set_user("customer_john")mem-llm check

```



---

print("ğŸ’¬ John: My order #12345 is delayed")# View statistics

## ğŸ”¥ Supported Models

response = agent.chat("My order #12345 is delayed")mem-llm stats

Works with any [Ollama](https://ollama.ai/) model. Recommended models:

print(f"ğŸ¤– Bot: {response}\n")

| Model | Size | Speed | Quality | Best For |

|-------|------|-------|---------|----------|# Export user data

| `granite4:tiny-h` | 2.5GB | âš¡âš¡âš¡ | â­â­ | Quick testing |

| `llama2` | 4GB | âš¡âš¡ | â­â­â­ | General use |# Customer 2 - SEPARATE MEMORY!mem-llm export john --format json --output data.json

| `mistral` | 4GB | âš¡âš¡ | â­â­â­â­ | Balanced |

| `llama3` | 5GB | âš¡ | â­â­â­â­â­ | Best quality |print("=" * 60)



```bashprint("ğŸ‘¤ Customer 2: Sarah")# Get help

# Download a model

ollama pull <model-name>print("=" * 60)mem-llm --help



# List installed modelsagent.set_user("customer_sarah")```

ollama list

```



---print("ğŸ’¬ Sarah: I want to return item #67890")**Available CLI Commands:**



## ğŸ“¦ Requirementsresponse = agent.chat("I want to return item #67890")



- Python 3.8+print(f"ğŸ¤– Bot: {response}\n")| Command | Description | Example |

- [Ollama](https://ollama.ai/) (for LLM)

- Minimum 4GB RAM|---------|-------------|---------|

- 5GB disk space

# Back to Customer 1 - remembers previous conversation!| `chat` | Interactive chat session | `mem-llm chat --user alice` |

**Python Dependencies (auto-installed):**

- `requests >= 2.31.0`print("=" * 60)| `check` | Verify system setup | `mem-llm check` |

- `pyyaml >= 6.0.1`

- `click >= 8.1.0`print("ğŸ‘¤ Back to Customer 1: John")| `stats` | Show statistics | `mem-llm stats --user john` |



---print("=" * 60)| `export` | Export user data | `mem-llm export john` |



## ğŸ› Troubleshootingagent.set_user("customer_john")| `clear` | Delete user data | `mem-llm clear john` |



### Ollama not running?



```bashprint("ğŸ’¬ John: What was my order number?")### Basic Chat

ollama serve

```response = agent.chat("What was my order number?")=======



### Model not found error?print(f"ğŸ¤– Bot: {response}")| Ã–zellik | AÃ§Ä±klama |



```bash```|---------|----------|

# Download the model

ollama pull granite4:tiny-h| ğŸ§  **KalÄ±cÄ± hafÄ±za** | Her kullanÄ±cÄ±nÄ±n sohbet geÃ§miÅŸi saklanÄ±r |



# Check installed models**Output:**| ğŸ‘¥ **Ã‡oklu kullanÄ±cÄ±** | Her kullanÄ±cÄ± iÃ§in ayrÄ± hafÄ±za yÃ¶netimi |

ollama list

``````| ğŸ”’ **Gizlilik** | Tamamen yerel Ã§alÄ±ÅŸÄ±r, buluta veri gÃ¶ndermez |



### Connection error?============================================================| âš¡ **HÄ±zlÄ±** | Hafif SQLite veya JSON depolama seÃ§enekleri |



```bashğŸ‘¤ Customer 1: John| ğŸ¯ **Kolay kullanÄ±m** | ÃœÃ§ satÄ±rda Ã§alÄ±ÅŸan Ã¶rnek |

# Check if Ollama is running

curl http://localhost:11434============================================================| ğŸ“š **Bilgi tabanÄ±** | Ek yapÄ±landÄ±rma olmadan dÃ¶kÃ¼manlardan bilgi yÃ¼kleme |



# Restart OllamağŸ’¬ John: My order #12345 is delayed| ğŸŒ **TÃ¼rkÃ§e desteÄŸi** | TÃ¼rkÃ§e diyaloglarda doÄŸal sonuÃ§lar |

ollama serve

```ğŸ¤– Bot: I'll help you check your order status...| ğŸ› ï¸ **AraÃ§ entegrasyonu** | GeliÅŸmiÅŸ araÃ§ sistemi ile geniÅŸletilebilir |



### Import error?



```bash============================================================---

# Upgrade to latest version

pip install --upgrade mem-llmğŸ‘¤ Customer 2: Sarah

```

============================================================## ğŸ§‘â€ğŸ« Tutorial

> If issues persist, run `mem-llm check` or `agent.check_setup()` and share the output when opening an issue.

ğŸ’¬ Sarah: I want to return item #67890

---

ğŸ¤– Bot: I can help you with the return process...TamamlanmÄ±ÅŸ Ã¶rnekleri adÄ±m adÄ±m incelemek iÃ§in [examples](examples) klasÃ¶rÃ¼ndeki rehberleri izleyebilirsiniz. Bu dizinde hem temel kullanÄ±m senaryolarÄ± hem de ileri seviye entegrasyonlar yer alÄ±r. Ã–ne Ã§Ä±kan iÃ§erikler:

## ğŸ“„ License



MIT License - Free to use in personal and commercial projects.

============================================================- [Basic usage walkthrough](examples/basic_usage.py) â€“ ilk hafÄ±zalÄ± ajanÄ±n nasÄ±l oluÅŸturulacaÄŸÄ±nÄ± gÃ¶sterir.

---

ğŸ‘¤ Back to Customer 1: John- [Customer support workflow](examples/customer_support.py) â€“ Ã§ok kullanÄ±cÄ±lÄ± mÃ¼ÅŸteri destek senaryosu.

## ğŸ”— Links

============================================================- [Knowledge base ingestion](examples/knowledge_base.py) â€“ dokÃ¼manlardan bilgi yÃ¼kleme.

- **PyPI:** https://pypi.org/project/mem-llm/

- **GitHub:** https://github.com/emredeveloper/Mem-LLMğŸ’¬ John: What was my order number?

- **Ollama:** https://ollama.ai/

- **Documentation:** [GitHub Wiki](https://github.com/emredeveloper/Mem-LLM/wiki)ğŸ¤– Bot: Your order number is #12345, which you mentioned was delayed.Her dosyada kodun yanÄ±nda aÃ§Ä±klamalar bulunur; komutlarÄ± kopyalayÄ±p Ã§alÄ±ÅŸtÄ±rarak sonuÃ§larÄ± deneyimleyebilirsiniz.



---```



## ğŸŒŸ Support Us## ğŸ“– KullanÄ±m Ã¶rnekleri



If you find this project useful, please â­ [star it on GitHub](https://github.com/emredeveloper/Mem-LLM)!---



---### Basic conversation



## ğŸ¤ Contributing### Example 3: Turkish Language Support>>>>>>> f002396c8c531e4cde33d19ac6a755494b1b30cd



Contributions are welcome! Please feel free to submit a Pull Request.



1. Fork the repository```python```python

2. Create your feature branch (`git checkout -b feature/AmazingFeature`)

3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)from mem_llm import MemAgentfrom mem_llm import MemAgent

4. Push to the branch (`git push origin feature/AmazingFeature`)

5. Open a Pull Request



---agent = MemAgent()agent = MemAgent()



<div align="center">agent.set_user("alice")

Made with â¤ï¸ by <a href="https://github.com/emredeveloper">C. Emre KarataÅŸ</a>

</div>print("ğŸ‡¹ğŸ‡· TÃ¼rkÃ§e KonuÅŸma Ã–rneÄŸi")


print("=" * 60)# Ä°lk konuÅŸma

agent.chat("I love pizza")

agent.set_user("ahmet")

# Later on...

print("ğŸ’¬ KullanÄ±cÄ±: Benim adÄ±m Ahmet ve Ä°stanbul'da yaÅŸÄ±yorum")agent.chat("What's my favorite food?")

response = agent.chat("Benim adÄ±m Ahmet ve Ä°stanbul'da yaÅŸÄ±yorum")# â†’ "Your favorite food is pizza"

print(f"ğŸ¤– Bot: {response}\n")```



print("ğŸ’¬ KullanÄ±cÄ±: Nerede yaÅŸÄ±yorum?")<<<<<<< HEAD

response = agent.chat("Nerede yaÅŸÄ±yorum?")### Multi-language Support

print(f"ğŸ¤– Bot: {response}\n")

```python

print("ğŸ’¬ KullanÄ±cÄ±: AdÄ±mÄ± hatÄ±rlÄ±yor musun?")# Works with any language

response = agent.chat("AdÄ±mÄ± hatÄ±rlÄ±yor musun?")=======

print(f"ğŸ¤– Bot: {response}")### Turkish language support

```

```python

**Output:**# Handles Turkish dialogue naturally

```>>>>>>> f002396c8c531e4cde33d19ac6a755494b1b30cd

ğŸ‡¹ğŸ‡· TÃ¼rkÃ§e KonuÅŸma Ã–rneÄŸiagent.set_user("ahmet")

============================================================agent.chat("Benim adÄ±m Ahmet ve pizza seviyorum")

ğŸ’¬ KullanÄ±cÄ±: Benim adÄ±m Ahmet ve Ä°stanbul'da yaÅŸÄ±yorumagent.chat("AdÄ±mÄ± hatÄ±rlÄ±yor musun?")

ğŸ¤– Bot: Memnun oldum Ahmet! Ä°stanbul gÃ¼zel bir ÅŸehir...# â†’ "Evet, adÄ±nÄ±z Ahmet!"

```

ğŸ’¬ KullanÄ±cÄ±: Nerede yaÅŸÄ±yorum?

ğŸ¤– Bot: Ä°stanbul'da yaÅŸÄ±yorsunuz.### Customer service scenario



ğŸ’¬ KullanÄ±cÄ±: AdÄ±mÄ± hatÄ±rlÄ±yor musun?```python

ğŸ¤– Bot: Evet, adÄ±nÄ±z Ahmet!agent = MemAgent()

```

# MÃ¼ÅŸteri 1

---agent.set_user("customer_001")

agent.chat("My order #12345 is delayed")

### Example 4: User Profile Extraction

# Customer 2 (separate memory!)

```pythonagent.set_user("customer_002")

from mem_llm import MemAgentagent.chat("I want to return item #67890")

```

agent = MemAgent()

agent.set_user("alice")### Inspecting the user profile



print("ğŸ“ Building user profile...")```python

print("=" * 60)# Retrieve automatically extracted user information

profile = agent.get_user_profile()

# Have natural conversations# {'name': 'Alice', 'favorite_food': 'pizza', 'location': 'NYC'}

conversations = [```

    "My name is Alice and I'm 28 years old",

    "I live in New York City",---

    "I work as a software engineer",

    "My favorite food is pizza"## ğŸ”§ YapÄ±landÄ±rma seÃ§enekleri

]

### JSON hafÄ±za (varsayÄ±lan ve basit)

for msg in conversations:

    print(f"ğŸ’¬ User: {msg}")```python

    response = agent.chat(msg)agent = MemAgent(

    print(f"ğŸ¤– Bot: {response}\n")    model="granite4:tiny-h",

    use_sql=False,  # JSON dosyalarÄ± ile hafÄ±za

# Extract profile automatically    memory_dir="memories"

print("=" * 60))

print("ğŸ“Š Extracted User Profile:")```

print("=" * 60)

profile = agent.get_user_profile()### SQL hafÄ±za (geliÅŸmiÅŸ ve hÄ±zlÄ±)



for key, value in profile.items():```python

    print(f"   {key}: {value}")agent = MemAgent(

```    model="granite4:tiny-h",

    use_sql=True,  # SQLite tabanlÄ± hafÄ±za

**Output:**    memory_dir="memories.db"

```)

ğŸ“ Building user profile...```

============================================================

ğŸ’¬ User: My name is Alice and I'm 28 years old### DiÄŸer Ã¶zelleÅŸtirmeler

ğŸ¤– Bot: Nice to meet you, Alice!...

```python

ğŸ’¬ User: I live in New York Cityagent = MemAgent(

ğŸ¤– Bot: New York City is a vibrant place...    model="llama2",  # Herhangi bir Ollama modeli

    ollama_url="http://localhost:11434"

ğŸ’¬ User: I work as a software engineer)

ğŸ¤– Bot: That's an interesting career...```



ğŸ’¬ User: My favorite food is pizza---

ğŸ¤– Bot: Pizza is delicious!...

## ğŸ“š API referansÄ±

============================================================

ğŸ“Š Extracted User Profile:### `MemAgent`

============================================================

   name: Alice```python

   age: 28# Initialize

   location: New York Cityagent = MemAgent(model="granite4:tiny-h", use_sql=False)

   occupation: Software Engineer

   favorite_food: Pizza# Set active user

```agent.set_user(user_id: str, name: Optional[str] = None)



---# Chat

response = agent.chat(message: str, metadata: Optional[Dict] = None) -> str

### Example 5: Complete Customer Service Workflow

# Get profile

```pythonprofile = agent.get_user_profile(user_id: Optional[str] = None) -> Dict

from mem_llm import MemAgent

# System check

# Initialize customer service agentstatus = agent.check_setup() -> Dict

print("ğŸ¢ Customer Service Bot Initializing...")```

agent = MemAgent(use_sql=True)  # SQL for better performance

---

# Simulate customer support session

def handle_customer(customer_id, customer_name):## ğŸ—‚ Bilgi tabanÄ± ve dokÃ¼manlardan yapÄ±landÄ±rma

    print("\n" + "=" * 70)

    print(f"ğŸ“ New Customer Session: {customer_name} (ID: {customer_id})")Kurumsal dokÃ¼manlarÄ±nÄ±zdan otomatik `config.yaml` Ã¼retin:

    print("=" * 70)

    ```python

    agent.set_user(customer_id, name=customer_name)from mem_llm import create_config_from_document

    

    # Customer introduces issue# PDF'den config.yaml Ã¼retin

    print(f"\nğŸ’¬ {customer_name}: Hi, my order hasn't arrived yet")create_config_from_document(

    response = agent.chat("Hi, my order hasn't arrived yet")    doc_path="company_info.pdf",

    print(f"ğŸ¤– Support: {response}")    output_path="config.yaml",

        company_name="Acme Corp"

    # Ask for details)

    print(f"\nğŸ’¬ {customer_name}: My order number is #45678")

    response = agent.chat("My order number is #45678")# OluÅŸan yapÄ±landÄ±rmayÄ± kullanÄ±n

    print(f"ğŸ¤– Support: {response}")agent = MemAgent(config_file="config.yaml")

    ```

    # Follow up later in conversation

    print(f"\nğŸ’¬ {customer_name}: Can you remind me what we were discussing?")---

    response = agent.chat("Can you remind me what we were discussing?")

    print(f"ğŸ¤– Support: {response}")## ğŸ”¥ Desteklenen modeller



# Handle multiple customers[Ollama](https://ollama.ai/) Ã¼zerindeki tÃ¼m modellerle Ã§alÄ±ÅŸÄ±r. Tavsiye edilen modeller:

handle_customer("cust_001", "Emma")

handle_customer("cust_002", "Michael")| Model | Size | Speed | Quality |

|-------|------|-------|---------|

# Return to first customer - memory persists!| `granite4:tiny-h` | 2.5GB | âš¡âš¡âš¡ | â­â­ |

print("\n" + "=" * 70)| `llama2` | 4GB | âš¡âš¡ | â­â­â­ |

print("ğŸ“ Returning Customer: Emma (ID: cust_001)")| `mistral` | 4GB | âš¡âš¡ | â­â­â­â­ |

print("=" * 70)| `llama3` | 5GB | âš¡ | â­â­â­â­â­ |

agent.set_user("cust_001")

```bash

print("\nğŸ’¬ Emma: What was my order number again?")ollama pull <model-name>

response = agent.chat("What was my order number again?")```

print(f"ğŸ¤– Support: {response}")

# Output: "Your order number is #45678"---

```

## ğŸ“¦ Gereksinimler

**Output:**

```- Python 3.8+

ğŸ¢ Customer Service Bot Initializing...- Ollama (LLM iÃ§in)

- Minimum 4GB RAM

======================================================================- 5GB disk alanÄ±

ğŸ“ New Customer Session: Emma (ID: cust_001)

======================================================================**Kurulum ile gelen baÄŸÄ±mlÄ±lÄ±klar:**

- `requests >= 2.31.0`

ğŸ’¬ Emma: Hi, my order hasn't arrived yet- `pyyaml >= 6.0.1`

ğŸ¤– Support: I'm sorry to hear that. I'll help you track your order...- `sqlite3` (Python ile birlikte gelir)



ğŸ’¬ Emma: My order number is #45678---

ğŸ¤– Support: Thank you for providing order #45678. Let me check...

## ğŸ› SÄ±k karÅŸÄ±laÅŸÄ±lan problemler

ğŸ’¬ Emma: Can you remind me what we were discussing?

ğŸ¤– Support: We're discussing your order #45678 that hasn't arrived yet...### Ollama Ã§alÄ±ÅŸmÄ±yor mu?



======================================================================```bash

ğŸ“ New Customer Session: Michael (ID: cust_002)ollama serve

======================================================================```



ğŸ’¬ Michael: Hi, my order hasn't arrived yet### Model bulunamadÄ± hatasÄ± mÄ± alÄ±yorsunuz?

ğŸ¤– Support: I'm sorry to hear that. I'll help you track your order...

```bash

ğŸ’¬ Michael: My order number is #78901ollama pull granite4:tiny-h

ğŸ¤– Support: Thank you for providing order #78901...```



======================================================================### ImportError veya baÄŸlantÄ± hatasÄ± mÄ± var?

ğŸ“ Returning Customer: Emma (ID: cust_001)

======================================================================```bash

pip install --upgrade mem-llm

ğŸ’¬ Emma: What was my order number again?```

ğŸ¤– Support: Your order number is #45678.

```> HÃ¢lÃ¢ sorun yaÅŸÄ±yorsanÄ±z `agent.check_setup()` Ã§Ä±ktÄ±sÄ±nÄ± ve hata mesajÄ±nÄ± issue aÃ§arken paylaÅŸÄ±n.



------



## ğŸ”§ Configuration Options## ğŸ“„ Lisans



### JSON Memory (Simple, Default)MIT LisansÄ± â€” kiÅŸisel veya ticari projelerinizde Ã¶zgÃ¼rce kullanabilirsiniz.



```python---

agent = MemAgent(

    model="granite4:tiny-h",## ğŸ”— FaydalÄ± baÄŸlantÄ±lar

    use_sql=False,  # JSON file-based memory

    memory_dir="memories"- **PyPI:** https://pypi.org/project/mem-llm/

)- **GitHub:** https://github.com/emredeveloper/Mem-LLM

```- **Ollama:** https://ollama.ai/



### SQL Memory (Advanced, Recommended for Production)---



```python## ğŸŒŸ Bize destek olun

agent = MemAgent(

    model="granite4:tiny-h",Proje iÅŸinize yaradÄ±ysa [GitHub](https://github.com/emredeveloper/Mem-LLM) Ã¼zerinden â­ vermeyi unutmayÄ±n!

    use_sql=True,  # SQLite-based memory

    memory_dir="memories.db"---

)

```<div align="center">

Sevgiyle geliÅŸtirildi â€” <a href="https://github.com/emredeveloper">C. Emre KarataÅŸ</a>

### Custom Configuration</div>


```python
agent = MemAgent(
    model="llama2",  # Any Ollama model
    ollama_url="http://localhost:11434",
    check_connection=True  # Verify setup on startup
)
```

---

## ğŸ› ï¸ Command Line Interface

```bash
# Start interactive chat
mem-llm chat --user john

# Check system status
mem-llm check

# View statistics
mem-llm stats

# Export user data
mem-llm export john --format json

# Clear user data
mem-llm clear john

# Get help
mem-llm --help
```

---

## ğŸ”„ Memory Backend Comparison

| Feature | JSON Mode | SQL Mode |
|---------|-----------|----------|
| **Setup** | âœ… Zero config | âš™ï¸ Minimal config |
| **Conversation Memory** | âœ… Yes | âœ… Yes |
| **User Profiles** | âœ… Yes | âœ… Yes |
| **Knowledge Base** | âŒ No | âœ… Yes |
| **Advanced Search** | âŒ No | âœ… Yes |
| **Multi-user Performance** | â­â­ Good | â­â­â­ Excellent |
| **Best For** | ğŸ  Personal use | ğŸ¢ Business use |

**Recommendation:**
- **JSON Mode**: Perfect for personal assistants and quick prototypes
- **SQL Mode**: Ideal for customer service, multi-user apps, and production

---

## ğŸ“š API Reference

### MemAgent Class

```python
# Initialize
agent = MemAgent(
    model="granite4:tiny-h",
    use_sql=True,
    memory_dir=None,
    ollama_url="http://localhost:11434",
    check_connection=False
)

# Set active user
agent.set_user(user_id: str, name: Optional[str] = None)

# Chat (returns response string)
response = agent.chat(message: str, metadata: Optional[Dict] = None) -> str

# Get user profile (auto-extracted from conversations)
profile = agent.get_user_profile(user_id: Optional[str] = None) -> Dict

# System check
status = agent.check_setup() -> Dict
```

---

## ğŸ”¥ Supported Models

Works with any [Ollama](https://ollama.ai/) model. Recommended models:

| Model | Size | Speed | Quality | Best For |
|-------|------|-------|---------|----------|
| `granite4:tiny-h` | 2.5GB | âš¡âš¡âš¡ | â­â­ | Quick testing |
| `llama2` | 4GB | âš¡âš¡ | â­â­â­ | General use |
| `mistral` | 4GB | âš¡âš¡ | â­â­â­â­ | Balanced |
| `llama3` | 5GB | âš¡ | â­â­â­â­â­ | Best quality |

```bash
# Download a model
ollama pull <model-name>

# List installed models
ollama list
```

---

## ğŸ“¦ Requirements

- Python 3.8+
- [Ollama](https://ollama.ai/) (for LLM)
- Minimum 4GB RAM
- 5GB disk space

**Python Dependencies (auto-installed):**
- `requests >= 2.31.0`
- `pyyaml >= 6.0.1`
- `click >= 8.1.0`

---

## ğŸ› Troubleshooting

### Ollama not running?

```bash
ollama serve
```

### Model not found error?

```bash
# Download the model
ollama pull granite4:tiny-h

# Check installed models
ollama list
```

### Connection error?

```bash
# Check if Ollama is running
curl http://localhost:11434

# Restart Ollama
ollama serve
```

### Import error?

```bash
# Upgrade to latest version
pip install --upgrade mem-llm
```

> If issues persist, run `mem-llm check` or `agent.check_setup()` and share the output when opening an issue.

---

## ğŸ“„ License

MIT License - Free to use in personal and commercial projects.

---

## ğŸ”— Links

- **PyPI:** https://pypi.org/project/mem-llm/
- **GitHub:** https://github.com/emredeveloper/Mem-LLM
- **Ollama:** https://ollama.ai/
- **Documentation:** [GitHub Wiki](https://github.com/emredeveloper/Mem-LLM/wiki)

---

## ğŸŒŸ Support Us

If you find this project useful, please â­ [star it on GitHub](https://github.com/emredeveloper/Mem-LLM)!

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

---

<div align="center">
Made with â¤ï¸ by <a href="https://github.com/emredeveloper">C. Emre KarataÅŸ</a>
</div>
