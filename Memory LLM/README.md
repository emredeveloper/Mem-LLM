# ğŸ§  Mem-Agent: BelleÄŸi Olan Mini Asistan

Sadece **4 milyar parametreli** yerel bir LLM ile Ã§alÄ±ÅŸan, kullanÄ±cÄ± etkileÅŸimlerini hatÄ±rlayan ve baÄŸlam farkÄ±ndalÄ±ÄŸÄ± ile cevap veren yapay zeka asistanÄ±.

## ğŸ¯ Proje AmacÄ±

BÃ¼yÃ¼k dil modellerinin (LLM) Ã§oÄŸu her konuÅŸmayÄ± "yeni" olarak gÃ¶rÃ¼r ve geÃ§miÅŸ etkileÅŸimleri hatÄ±rlamaz. Mem-Agent, **yerel olarak Ã§alÄ±ÅŸan kÃ¼Ã§Ã¼k bir model** kullanarak:

- ğŸ§  KullanÄ±cÄ± etkileÅŸimlerini hatÄ±rlar
- ğŸ’¾ KonuÅŸma geÃ§miÅŸini saklar
- ğŸ¯ BaÄŸlam farkÄ±ndalÄ±ÄŸÄ± ile cevaplar
- ğŸ  Tamamen yerel ve gizli Ã§alÄ±ÅŸÄ±r

## ğŸ“¦ Kurulum

### 1. Ollama Kurulumu

```bash
# Ollama'yÄ± yÃ¼kleyin (https://ollama.ai/)
# Windows: ollama.exe installer
# Mac: brew install ollama
# Linux: curl https://ollama.ai/install.sh | sh

# Ollama servisini baÅŸlatÄ±n
ollama serve
```

### 2. Model Ä°ndirme

```bash
# Granite4 tiny model'i indirin (yaklaÅŸÄ±k 2.5 GB)
ollama pull granite4:tiny-h
```

### 3. Mem-Agent Kurulumu

```bash
# Gerekli kÃ¼tÃ¼phaneleri yÃ¼kleyin
pip install -r requirements.txt

# Veya development modu iÃ§in:
pip install -e .
```

## ğŸš€ HÄ±zlÄ± BaÅŸlangÄ±Ã§

### Basit KullanÄ±m

```python
from mem_agent import MemAgent

# Agent oluÅŸtur
agent = MemAgent(model="granite4:tiny-h")

# Sistem kontrolÃ¼
status = agent.check_setup()
if status['status'] == 'ready':
    print("âœ… Sistem hazÄ±r!")
else:
    print("âŒ Hata:", status)

# KullanÄ±cÄ± ayarla
agent.set_user("user123")

# Ä°lk konuÅŸma
response = agent.chat("Merhaba, benim adÄ±m Ali")
print(response)

# Ä°kinci konuÅŸma - Beni hatÄ±rlÄ±yor!
response = agent.chat("AdÄ±mÄ± hatÄ±rlÄ±yor musun?")
print(response)
```

### MÃ¼ÅŸteri Hizmetleri Ã–rneÄŸi

```python
from mem_agent import MemAgent

agent = MemAgent()
agent.set_user("ahmet123")

# Ä°lk gÃ¼n
response = agent.chat(
    "SipariÅŸim nerede?",
    metadata={"order": "#12345", "issue": "kargo gecikmesi"}
)

# 3 gÃ¼n sonra - GeÃ§miÅŸi hatÄ±rlÄ±yor
response = agent.chat("Merhaba yine ben")
# Bot: "Merhaba Ahmet! #12345 numaralÄ± sipariÅŸinizle ilgilenmiÅŸtim..."
```

## ğŸ“š Ã–rnek Scriptler

### 1. Basit Test

```bash
python example_simple.py
```

### 2. MÃ¼ÅŸteri Hizmetleri SimÃ¼lasyonu

```bash
python example_customer_service.py
```

## ğŸ—ï¸ Proje YapÄ±sÄ±

```
Memory LLM/
â”œâ”€â”€ __init__.py              # Paket baÅŸlatma
â”œâ”€â”€ mem_agent.py             # Ana asistan sÄ±nÄ±fÄ±
â”œâ”€â”€ memory_manager.py        # Bellek yÃ¶netimi
â”œâ”€â”€ llm_client.py            # Ollama entegrasyonu
â”œâ”€â”€ example_simple.py        # Basit Ã¶rnek
â”œâ”€â”€ example_customer_service.py  # MÃ¼ÅŸteri hizmetleri Ã¶rneÄŸi
â”œâ”€â”€ setup.py                 # Kurulum scripti
â”œâ”€â”€ requirements.txt         # BaÄŸÄ±mlÄ±lÄ±klar
â””â”€â”€ README.md               # Bu dosya
```

## ğŸ”§ API KullanÄ±mÄ±

### MemAgent SÄ±nÄ±fÄ±

```python
from mem_agent import MemAgent

agent = MemAgent(
    model="granite4:tiny-h",           # Ollama model adÄ±
    memory_dir="memories",             # Bellek dizini
    ollama_url="http://localhost:11434" # Ollama API URL
)
```

#### Temel Metodlar

```python
# KullanÄ±cÄ± ayarla
agent.set_user("user_id")

# Sohbet et
response = agent.chat(
    message="Merhaba",
    user_id="optional_user_id",  # set_user kullanÄ±lmadÄ±ysa
    metadata={"key": "value"}     # Ek bilgiler
)

# Bellek Ã¶zeti al
summary = agent.memory_manager.get_summary("user_id")

# GeÃ§miÅŸte ara
results = agent.search_user_history("anahtar_kelime", "user_id")

# Profil gÃ¼ncelle
agent.update_user_info({
    "name": "Ali",
    "preferences": {"language": "tr"}
})

# Ä°statistikler
stats = agent.get_statistics()

# BelleÄŸi export et
json_data = agent.export_memory("user_id")

# BelleÄŸi temizle (DÄ°KKAT!)
agent.clear_user_memory("user_id", confirm=True)
```

### MemoryManager SÄ±nÄ±fÄ±

```python
from memory_manager import MemoryManager

memory = MemoryManager(memory_dir="memories")

# Bellek yÃ¼kle
data = memory.load_memory("user_id")

# EtkileÅŸim ekle
memory.add_interaction(
    user_id="user_id",
    user_message="Merhaba",
    bot_response="Merhaba! NasÄ±l yardÄ±mcÄ± olabilirim?",
    metadata={"timestamp": "2025-10-13"}
)

# Son konuÅŸmalarÄ± al
recent = memory.get_recent_conversations("user_id", limit=5)

# Arama
results = memory.search_memory("user_id", "sipariÅŸ")
```

### OllamaClient SÄ±nÄ±fÄ±

```python
from llm_client import OllamaClient

client = OllamaClient(model="granite4:tiny-h")

# Basit Ã¼retim
response = client.generate("Merhaba dÃ¼nya!")

# Sohbet formatÄ±
response = client.chat([
    {"role": "system", "content": "Sen yardÄ±msever bir asistansÄ±n"},
    {"role": "user", "content": "Merhaba"}
])

# BaÄŸlantÄ± kontrolÃ¼
is_ready = client.check_connection()

# Model listesi
models = client.list_models()
```

## ğŸ’¡ KullanÄ±m SenaryolarÄ±

### 1. MÃ¼ÅŸteri Hizmetleri Botu
- MÃ¼ÅŸteri geÃ§miÅŸini hatÄ±rlar
- Ã–nceki sorunlarÄ± bilir
- KiÅŸiselleÅŸtirilmiÅŸ Ã¶neriler yapar

### 2. KiÅŸisel Asistan
- GÃ¼nlÃ¼k aktiviteleri takip eder
- Tercihleri Ã¶ÄŸrenir
- HatÄ±rlatmalar yapar

### 3. EÄŸitim AsistanÄ±
- Ã–ÄŸrenci ilerlemesini takip eder
- Zorluk seviyesini ayarlar
- GeÃ§miÅŸ hatalarÄ± hatÄ±rlar

### 4. Destek Ticket Sistemi
- Ticket geÃ§miÅŸini saklar
- Ä°lgili eski ticket'larÄ± bulur
- Ã‡Ã¶zÃ¼m Ã¶nerileri sunar

## ğŸ“Š Bellek FormatÄ±

Bellekler JSON formatÄ±nda saklanÄ±r:

```json
{
  "conversations": [
    {
      "timestamp": "2025-10-13T10:30:00",
      "user_message": "Merhaba",
      "bot_response": "Merhaba! NasÄ±l yardÄ±mcÄ± olabilirim?",
      "metadata": {
        "topic": "selamlama"
      }
    }
  ],
  "profile": {
    "user_id": "user123",
    "first_seen": "2025-10-13T10:30:00",
    "preferences": {},
    "summary": {}
  },
  "last_updated": "2025-10-13T10:35:00"
}
```

## ğŸ”’ Gizlilik ve GÃ¼venlik

- âœ… Tamamen yerel Ã§alÄ±ÅŸÄ±r (internet baÄŸlantÄ±sÄ± gerektirmez)
- âœ… Veriler bilgisayarÄ±nÄ±zda saklanÄ±r
- âœ… ÃœÃ§Ã¼ncÃ¼ parti servislere veri gÃ¶nderilmez
- âœ… Bellekler JSON formatÄ±nda, kolayca silinebilir

## ğŸ› ï¸ GeliÅŸtirme

### Test Modu

```python
# Belleksiz basit sohbet (test iÃ§in)
response = agent.simple_chat("Test mesajÄ±")
```

### Kendi Modelinizi Kullanma

```python
# FarklÄ± bir Ollama modeli
agent = MemAgent(model="llama2:7b")

# Veya baÅŸka bir LLM API
# llm_client.py dosyasÄ±nÄ± Ã¶zelleÅŸtirin
```

## ğŸ› Sorun Giderme

### Ollama BaÄŸlantÄ± HatasÄ±

```bash
# Ollama servisini baÅŸlatÄ±n
ollama serve

# Port kontrolÃ¼
netstat -an | findstr "11434"
```

### Model BulunamadÄ±

```bash
# Model listesini kontrol edin
ollama list

# Model indirin
ollama pull granite4:tiny-h
```

### Bellek SorunlarÄ±

```python
# Bellek dizinini kontrol edin
import os
os.path.exists("memories")

# Bellek dosyalarÄ±nÄ± listeleyin
os.listdir("memories")
```

## ğŸ“ˆ Performans

- **Model Boyutu**: ~2.5 GB
- **YanÄ±t SÃ¼resi**: ~1-3 saniye (CPU'ya baÄŸlÄ±)
- **Bellek KullanÄ±mÄ±**: ~4-6 GB RAM
- **Disk KullanÄ±mÄ±**: KullanÄ±cÄ± baÅŸÄ±na ~10-50 KB

## ğŸ¤ KatkÄ±da Bulunma

1. Fork yapÄ±n
2. Feature branch oluÅŸturun (`git checkout -b feature/amazing-feature`)
3. Commit yapÄ±n (`git commit -m 'feat: Add amazing feature'`)
4. Push edin (`git push origin feature/amazing-feature`)
5. Pull Request aÃ§Ä±n

## ğŸ“ Lisans

MIT License - Detaylar iÃ§in LICENSE dosyasÄ±na bakÄ±n.

## ğŸ™ TeÅŸekkÃ¼rler

- [Ollama](https://ollama.ai/) - Yerel LLM sunucusu
- [Granite](https://www.ibm.com/granite) - IBM Granite modelleri

## ğŸ“ Ä°letiÅŸim

SorularÄ±nÄ±z iÃ§in issue aÃ§abilirsiniz.

---

**Not**: Bu proje eÄŸitim ve araÅŸtÄ±rma amaÃ§lÄ±dÄ±r. Ãœretim ortamÄ±nda kullanmadan Ã¶nce kapsamlÄ± test yapÄ±n.

